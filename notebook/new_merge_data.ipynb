{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignored_columns = [\n",
    "    \"EyeBlinkLeft\", \n",
    "    \"EyeBlinkRight\", \n",
    "    \"EyeLookDownLeft\", \n",
    "    \"EyeLookDownRight\", \n",
    "    \"EyeLookInLeft\", \n",
    "    \"EyeLookInRight\", \n",
    "    \"EyeLookOutLeft\", \n",
    "    \"EyeLookOutRight\", \n",
    "    \"EyeBlinkLeft\", \n",
    "    \"EyeBlinkRight\", \n",
    "    \"EyeLookDownLeft\", \n",
    "    \"EyeLookDownRight\", \n",
    "    \"EyeLookInLeft\", \n",
    "    \"EyeLookInRight\", \n",
    "    \"EyeLookOutLeft\", \n",
    "    \"EyeLookOutRight\", \n",
    "    \"EyeLookUpLeft\", \n",
    "    \"EyeLookUpRight\", \n",
    "    \"EyeSquintLeft\", \n",
    "    \"EyeSquintRight\", \n",
    "    \"EyeWideLeft\", \n",
    "    \"EyeWideRight\", \n",
    "    \"BrowDownLeft\", \n",
    "    \"BrowDownRight\", \n",
    "    \"BrowInnerUp\", \n",
    "    \"BrowOuterUpLeft\", \n",
    "    \"BrowOuterUpRight\", \n",
    "    \"CheekSquintLeft\", \n",
    "    \"CheekSquintRight\", \n",
    "    \"JawLeft\", \n",
    "    \"JawRight\", \n",
    "    \"MouthLeft\", \n",
    "    \"MouthRight\", \n",
    "    \"MouthUpperUpLeft\", \n",
    "    \"MouthUpperUpRight\", \n",
    "    \"MouthLowerDownLeft\", \n",
    "    \"MouthLowerDownRight\", \n",
    "    \"MouthSmileLeft\", \n",
    "    \"MouthSmileRight\", \n",
    "    \"MouthFrownLeft\", \n",
    "    \"MouthFrownRight\", \n",
    "    \"NoseSneerLeft\", \n",
    "    \"NoseSneerRight\", \n",
    "    \"HeadYaw\", \n",
    "    \"HeadPitch\", \n",
    "    \"HeadRoll\", \n",
    "    \"TongueOut\", \n",
    "    \"LeftEyeYaw\", \n",
    "    \"LeftEyePitch\", \n",
    "    \"LeftEyeRoll\", \n",
    "    \"RightEyeYaw\", \n",
    "    \"RightEyePitch\", \n",
    "    \"RightEyeRoll\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:00, 66.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7  sentences loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# audio_ggongggong\n",
    "indices = []\n",
    "spectrograms = []\n",
    "spectrogram_lengths = []\n",
    "\n",
    "# shape_ggongggong\n",
    "timecodes = []\n",
    "blendshapes = []\n",
    "blendshape_lengths = []\n",
    "f_names = []\n",
    "\n",
    "for essential in tqdm(os.scandir('/shared/air/shared/youngkim/mediazen/preprocessed/test/essentials/')):\n",
    "    \n",
    "    f_name = os.path.splitext(essential.name)[0]\n",
    "    idx = int(f_name.split('_')[0])\n",
    "    speaker = re.sub(r'[0-9]+', '', f_name.split('_')[2])\n",
    "\n",
    "    # spectrogram: torch.Tensor (audio_frame, 161)\n",
    "    # sample_rate: Int, 16000\n",
    "    # blendshape: Dict (Timecode, BlendShapeCount, *(BlendShapeColumns))\n",
    "    spectrogram, sample_rate, blendshape = torch.load(essential.path)\n",
    "    spectrogram_length = len(spectrogram)\n",
    "    # ignored_columns에 대문자 오타때문에 다시 걸러줌\n",
    "    for column in ignored_columns:\n",
    "        if column in blendshape.keys():\n",
    "            del(blendshape[column])\n",
    "    timecode = blendshape.pop('Timecode') # List (shape_frame)\n",
    "    blendshape_count = blendshape.pop('BlendShapeCount')[0] # Int, 61 -> 필요없음\n",
    "    blendshape_columns = list(blendshape.keys()) # List (num. of blendshape)\n",
    "    try:\n",
    "        blendshape_tensor = torch.Tensor(list(blendshape.values())).T # torch.Tensor (shape_frame, num. of blendshape)\n",
    "    except TypeError:\n",
    "        print('blendshape type error: ', essential.path)\n",
    "        continue\n",
    "    blendshape_length = len(blendshape_tensor)\n",
    "    \n",
    "\n",
    "    # error check\n",
    "    if sample_rate != 16000:\n",
    "        print('sample rate error: ', essential.path)\n",
    "        continue\n",
    "\n",
    "    if torch.sum(spectrogram.isnan()):\n",
    "        print('spectrogram nan error: ', essential.path)\n",
    "        continue\n",
    "    \n",
    "    if torch.sum(blendshape_tensor.isnan()):\n",
    "        print('blendshape nan error: ', essential.path)\n",
    "        break\n",
    "\n",
    "    # audio_ggongggong\n",
    "    indices.append(idx)\n",
    "    spectrograms.append(spectrogram)\n",
    "    spectrogram_lengths.append(spectrogram_length)\n",
    "    # shape_ggongggong\n",
    "    timecodes.append(timecode)\n",
    "    blendshapes.append(blendshape_tensor)\n",
    "    blendshape_lengths.append(blendshape_length)\n",
    "    f_names.append(f_name)\n",
    "    \n",
    "\n",
    "indices_tensor = torch.IntTensor(indices)\n",
    "padded_spectrogram_tensors = torch.nn.utils.rnn.pad_sequence(spectrograms, batch_first=True)\n",
    "spectrogram_length_tensors = torch.IntTensor(spectrogram_lengths)\n",
    "\n",
    "stripped_timecodes = [torch.LongTensor([int(time.replace(':', '').replace('.', '')) for time in timecode]) for timecode in timecodes]\n",
    "padded_timecodes = torch.nn.utils.rnn.pad_sequence(stripped_timecodes, batch_first=True)\n",
    "padded_blendshape_tensors = torch.nn.utils.rnn.pad_sequence(blendshapes, batch_first=True)\n",
    "blendshape_length_tensors = torch.IntTensor(blendshape_lengths)\n",
    "\n",
    "if len(padded_spectrogram_tensors) != len(padded_blendshape_tensors):\n",
    "    print(\"length error, audio and shape length doesn't match.\")\n",
    "else:\n",
    "    print(len(padded_spectrogram_tensors), ' sentences loaded.')\n",
    "\n",
    "spectrogram_data = (sample_rate, indices_tensor, padded_spectrogram_tensors, spectrogram_length_tensors)\n",
    "blendshape_data = (padded_timecodes, blendshape_count, blendshape_columns, padded_blendshape_tensors, blendshape_length_tensors, f_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = '/shared/air/shared/youngkim/mediazen/preprocessed/test/ggongggong'\n",
    "\n",
    "target_spec_data = os.path.join(target_dir, 'audio_ggongggong.pt')\n",
    "target_shape_data = os.path.join(target_dir, 'shape_ggongggong.pt')\n",
    "\n",
    "torch.save(spectrogram_data, target_spec_data)\n",
    "torch.save(blendshape_data, target_shape_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(blendshape_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "timecode = blendshape_data[0][0]\n",
    "\n",
    "recovered_timecode =  [f'{(s := str(time.item()))[:-9]}:{s[-9:-7]}:{s[-7:-5]}:{s[-5:-3]}.{s[-3:]}' for time in timecode]\n",
    "timecode_index = pd.Index(recovered_timecode, name='Timecode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column = blendshape_data[2]\n",
    "recovered_column = ['BlendShapeCount', *column]\n",
    "len(recovered_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['23:51:40:02.131', '23:51:40:03.131', '23:51:40:04.130',\n",
       "       '23:51:40:05.130', '23:51:40:06.129', '23:51:40:08.129',\n",
       "       '23:51:40:09.128', '23:51:40:10.128', '23:51:40:11.127',\n",
       "       '23:51:40:12.127',\n",
       "       ...\n",
       "       ':::.0', ':::.0', ':::.0', ':::.0', ':::.0', ':::.0', ':::.0', ':::.0',\n",
       "       ':::.0', ':::.0'],\n",
       "      dtype='object', name='Timecode', length=7012)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "idx = 0\n",
    "length = blendshape_data[4][idx]\n",
    "index = spectrogram_data[1][idx]\n",
    "timecode = blendshape_data[0][idx]\n",
    "column = blendshape_data[2]\n",
    "\n",
    "\n",
    "# def save_to_csv(self, length, index, timecode, column, prediction):\n",
    "recovered_timecode =  [f'{(s := str(time.item()))[:-9]}:{s[-9:-7]}:{s[-7:-5]}:{s[-5:-3]}.{s[-3:]}' for time in timecode]\n",
    "timecode_index = pd.Index(recovered_timecode, name='Timecode')\n",
    "\n",
    "blendshape_count = np.expand_dims(np.full(len(timecode), len(column)), axis=1)\n",
    "filtered_prediction = np.apply_along_axis(self.column_filter, 0, prediction)\n",
    "recovered_content = np.hstack([blendshape_count, filtered_prediction])\n",
    "\n",
    "recovered_column = ['BlendShapeCount', *column]\n",
    "\n",
    "df = pd.DataFrame(recovered_content, index=timecode_index, columns=recovered_column)\n",
    "chopped_df = df[:length.item()]\n",
    "chopped_df.to_csv(os.path.join(self.target_dir, f'{index}_prediction.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_delete(tensor, remove_idx):\n",
    "    return torch.cat((tensor[:remove_idx], tensor[remove_idx+1:])).detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_idx = 1270"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "torch.Size([0])\n",
      "torch.Size([1443, 11682, 161])\n",
      "torch.Size([1442, 11682, 161])\n",
      "torch.Size([1443])\n",
      "torch.Size([1442])\n",
      "torch.Size([1443, 7012])\n",
      "torch.Size([1442, 7012])\n",
      "torch.Size([1443, 7012, 16])\n",
      "torch.Size([1442, 7012, 16])\n",
      "torch.Size([1443])\n",
      "torch.Size([1442])\n"
     ]
    }
   ],
   "source": [
    "new_spectrogram_data = list(spectrogram_data)\n",
    "\n",
    "for i in [1, 2, 3]:\n",
    "    print(new_spectrogram_data[i].shape)\n",
    "    new_spectrogram_data[i] = tensor_delete(new_spectrogram_data[i], remove_idx)\n",
    "    print(new_spectrogram_data[i].shape)\n",
    "\n",
    "new_spectrogram_data = tuple(new_spectrogram_data)\n",
    "\n",
    "new_blendshape_data = list(blendshape_data)\n",
    "\n",
    "for i in [0, 3, 4]:\n",
    "    print(new_blendshape_data[i].shape)\n",
    "    new_blendshape_data[i] = tensor_delete(new_blendshape_data[i], remove_idx)\n",
    "    print(new_blendshape_data[i].shape)\n",
    "\n",
    "new_blendshape_data = tuple(new_blendshape_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('torch18')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98f3181f4900a3173dff2251935f87d1f345563f3a07cb125f858d750ad52894"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
